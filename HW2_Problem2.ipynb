{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# code source from http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def initiate_centroids(data_array, k):\n",
    "    #initiate an array for the K indexes of picking the intitial K centroids \n",
    "    initial_indexes = random.sample(range(len(data_array)), k)\n",
    "    # return the array of data correspondoing to the k indexes\n",
    "    K_centroids = np.array(data_array)[initial_indexes]\n",
    "    \n",
    "    return K_centroids\n",
    "\n",
    "#def \n",
    "\n",
    "def K_means(data_array, k, iteration):\n",
    "    \n",
    "    # record the dimentions of the data array.\n",
    "    dimens = len(data_array[1])\n",
    "    \n",
    "    # initiate an array of K centroids \n",
    "    initial_cens = initiate_centroids(data_array, k)\n",
    "    \n",
    "    #initiate an list storing the new cluster for each data point (e.g.,row in the matrix)\n",
    "    new_clusters = [0]*len(data_array)\n",
    "    \n",
    "    while iteration>0:\n",
    "        \n",
    "        iteration -= 1\n",
    "        \n",
    "        ##### E Step\n",
    "        # calculate the similarity matrix \n",
    "        simi_matrix = euclidean_distances(data_array, initial_cens)\n",
    "        \n",
    "        # return the indexes of the minimum number in each row in the matrix, which is \n",
    "        # the culster lable that data point belong to\n",
    "        new_clusters = np.argmin(simi_matrix, axis=1)\n",
    "        \n",
    "        \n",
    "        old_cens = initial_cens\n",
    "        \n",
    "        # reset the initial k centroids and be prepared to stored the new ones\n",
    "        initial_cens = []\n",
    "        #initate the centroids with same dimensions while each dimens is 0\n",
    "        for i in range(k):\n",
    "            initial_cens.append(np.array([float(0)]*dimens))\n",
    "            \n",
    "        initial_cens = np.array(initial_cens)\n",
    "        \n",
    "        #define a list for recording each cluster's number\n",
    "        count = [0]*k\n",
    "        \n",
    "        # here is to loop each data point and aggregate the data's each feature colum(dimension)\n",
    "        # into a certain cluster, after that, initial_cens[k] is the aggretated location of each\n",
    "        # cluster\n",
    "        \n",
    "        ####### M Step\n",
    "        for j in range(len(data_array)):\n",
    "            # here initial_cens[new_clusters[j]] is to record the array that going to  add\n",
    "            # each dimension of each data point that belong to the jth new cluster \n",
    "            initial_cens[new_clusters[j]]+=np.array(data_array[j])\n",
    "            \n",
    "            # to add the number of data points in each cluster, here new_clusters[j] is the index\n",
    "            # of new cluster each data point belong to, thus count[new_clusters[j]] record \n",
    "            # the number of each cluster\n",
    "            count[new_clusters[j]] += 1\n",
    "            \n",
    "            #here is to get the mean location for the k culters,  namely, initial_cens[k] become \n",
    "            #the new culster's location\n",
    "        for t in range(k):\n",
    "            #get the mean\n",
    "            initial_cens[t] = initial_cens[t]/count[t]\n",
    "            \n",
    "            \n",
    "    \n",
    "    return initial_cens, new_clusters\n",
    "            \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purity and Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn code from this source: https://docs.python.org/2/library/collections.html#collections.Counter\n",
    "# A Counter is a dict subclass for counting hashable objects. \n",
    "from collections import Counter\n",
    "\n",
    "def compute_purity(data_array, cens, clusters, real_label):\n",
    "    # Define a dictionary for storing the key(cluster K) and value(the collection\n",
    "    # of the real lables of the points c in this cluster)\n",
    "    cluster_lable_dic = {}\n",
    "    \n",
    "    ## here we will gather the real lables of the points in a certain cluster, \n",
    "    ## format: cluster K : (K_real, K_real , K_real, J_real, B_real)\n",
    "    \n",
    "    # here we loop from each data point\n",
    "    for i in range(len(data_array)):\n",
    "        # clusters[i] return the cluster index of the predicted cluster for data point i \n",
    "        # if the cluster index (e.g., predicted label) been recorded, just add the point i's read-label \n",
    "        if clusters[i] in cluster_lable_dic.keys():\n",
    "            cluster_lable_dic[clusters[i]].append(real_label[i])\n",
    "        #else if the cluster index (e.g., predicted label) have't been recorded before,add here\n",
    "        else:\n",
    "            cluster_lable_dic[clusters[i]]= [real_label[i]]\n",
    "    \n",
    "    #iniitate total count as 0\n",
    "    count_total = 0\n",
    "    \n",
    "    for j in cluster_lable_dic.keys():\n",
    "        count_total += Counter(cluster_lable_dic[j]).most_common()[0][1] #return the number of the most common element\n",
    "    \n",
    "    purity = float(count_total)/float(len(data_array))\n",
    "    \n",
    "    return purity\n",
    "    \n",
    "\n",
    "def compute_gini(data_array, cens, clusters, real_label):\n",
    "    # Define a dictionary for storing the key(cluster K) and value(the collection\n",
    "    # of the real lables of the points c in this cluster)\n",
    "    cluster_lable_dic = {}\n",
    "    \n",
    "    ## here we will gather the real lables of the points in a certain cluster, \n",
    "    ## format: cluster K : (K_real, K_real , K_real, J_real, B_real)\n",
    "    \n",
    "    # here we loop from each data point\n",
    "    for i in range(len(data_array)):\n",
    "        # clusters[i] return the cluster index of the predicted cluster for data point i \n",
    "        # if the cluster index (e.g., predicted label) been recorded, just add the point i's read-label \n",
    "        if clusters[i] in cluster_lable_dic.keys():\n",
    "            cluster_lable_dic[clusters[i]].append(real_label[i])\n",
    "        #else if the cluster index (e.g., predicted label) have't been recorded before,add here\n",
    "        else:\n",
    "            cluster_lable_dic[clusters[i]]= [real_label[i]]\n",
    "    \n",
    "    # for each cluster, gini index =  aggregating the P(k)(1-P(k)), and P(k)= (num_in_k_cluster)/N\n",
    "    # thus gini = (num_in_k_cluster)*(N- (num_in_k_cluster))/ (N**2)\n",
    "    gini= 0\n",
    "    \n",
    "    for j in cluster_lable_dic.keys():\n",
    "        #return a list, e.g., for cluster K which there are 10 point in it: [(K,5), (J,3), (A,2)]\n",
    "        list_lable = Counter(cluster_lable_dic[j]).most_common()\n",
    "        # total number of data point in cluster j\n",
    "        total_num = len(cluster_lable_dic[j])\n",
    "        #total categories(cluster) appear in cluster j\n",
    "        total_cate = len(list_lable)\n",
    "        \n",
    "        for k in range(total_cate):\n",
    "            #list_lable[k][1] is the number of the lable(cluster) denoted as \"list_lable[k][1] \"\n",
    "            gini += float(list_lable[k][1]*(total_num - list_lable[k][1]))/float(total_num**2)\n",
    "    \n",
    "    gini = float(gini)/float(len(cluster_lable_dic.keys()))\n",
    "    \n",
    "    return gini\n",
    "        \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST, K = 5, purity is 0.45248571428571427 and gini is 0.5934476807517318\n",
      "MNIST, K = 10, purity is 0.5851571428571428 and gini is 0.5206361175480044\n",
      "MNIST, K = 20, purity is 0.7237 and gini is 0.37006016129846436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# source: http://scikit-learn.org/stable/datasets/index.html\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "#convert the MNIST dataset into array\n",
    "mnist_data = np.array(mnist['data'])\n",
    "\n",
    "cen_mnist_5, cluster_mnist_5 = K_means(mnist_data, 5, 50)\n",
    "cen_mnist_10, cluster_mnist_10 = K_means(mnist_data, 10, 50)\n",
    "cen_mnist_20, cluster_mnist_20 = K_means(mnist_data, 20, 50)\n",
    "\n",
    "\n",
    "purity_5 = compute_purity(mnist_data, cen_mnist_5, cluster_mnist_5, mnist.target)\n",
    "gini_5 = compute_gini(mnist_data, cen_mnist_5, cluster_mnist_5, mnist.target)\n",
    "purity_10 = compute_purity(mnist_data, cen_mnist_10, cluster_mnist_10, mnist.target)\n",
    "gini_10 = compute_gini(mnist_data, cen_mnist_10, cluster_mnist_10, mnist.target)\n",
    "purity_20 = compute_purity(mnist_data, cen_mnist_20, cluster_mnist_20, mnist.target)\n",
    "gini_20 = compute_gini(mnist_data, cen_mnist_20, cluster_mnist_20, mnist.target)\n",
    "\n",
    "print(\"MNIST, K = 5, purity is {} and gini is {}\".format(purity_5, gini_5))\n",
    "print(\"MNIST, K = 10, purity is {} and gini is {}\".format(purity_10, gini_10))\n",
    "print(\"MNIST, K = 20, purity is {} and gini is {}\".format(purity_20, gini_20))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deshenghu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "(11314, 130088)\n",
      "(7532, 130088)\n"
     ]
    }
   ],
   "source": [
    "# code source from http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#\n",
    "# code source from http://www.nltk.org/book/ch02.html\n",
    "# code source from http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# extracts the archive contents in the ~/scikit_learn_data/20news_home folder \n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "################# this part is for pre-processing the text file which isn't mandatory\n",
    "# get rid of the stopwords and punctuation\n",
    "nltk.download('stopwords')\n",
    "stop_word =stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "stopw_punctuation = list(stop_word) + list(punctuation)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# to tokenize the word\n",
    "for j in range(len(newsgroups_train.data)):\n",
    "    newsgroups_train.data[j] = \" \".join([w for w in word_tokenize(newsgroups_train.data[j]) if w not in stopw_punctuation])\n",
    "   # print(newsgroups_train.data[])\n",
    "for j in range(len(newsgroups_test.data)):\n",
    "    newsgroups_test.data[j] = \" \".join([w for w in word_tokenize(newsgroups_train.data[j]) if w not in stopw_punctuation])\n",
    "\n",
    "#print(len(newsgroups_train.data))\n",
    "#print(newsgroups_train.data[1])\n",
    "\n",
    "##############\n",
    "\n",
    "# from http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "# the test data set don't need to fit\n",
    "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
    "print(vectors_train.shape)\n",
    "print(vectors_test.shape)\n",
    "\n",
    "news_data = np.array(vectors_train.todense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20NEWS, K = 5, purity is 0.17367862824818808 and gini is 0.8771791867317184\n",
      "20NEWS, K = 10, purity is 0.2612692239703023 and gini is 0.7751934615060898\n",
      "20NEWS, K = 20, purity is 0.3261445996111013 and gini is 0.6095694928431954\n"
     ]
    }
   ],
   "source": [
    "cen_news_5, cluster_news_5 = K_means(news_data, 5, 100)\n",
    "cen_news_10, cluster_news_10 = K_means(news_data, 10, 100)\n",
    "cen_news_20, cluster_news_20 = K_means(news_data, 20, 100)\n",
    "\n",
    "purity_5 = compute_purity(news_data, cen_news_5, cluster_news_5, newsgroups_train.target)\n",
    "gini_5 = compute_gini(news_data, cen_news_5, cluster_news_5, newsgroups_train.target)\n",
    "purity_10 = compute_purity(news_data, cen_news_10, cluster_news_10, newsgroups_train.target)\n",
    "gini_10 = compute_gini(news_data, cen_news_10, cluster_news_10, newsgroups_train.target)\n",
    "purity_20 = compute_purity(news_data, cen_news_20, cluster_news_20, newsgroups_train.target)\n",
    "gini_20 = compute_gini(news_data, cen_news_20, cluster_news_20, newsgroups_train.target)\n",
    "\n",
    "print(\"20NEWS, K = 5, purity is {} and gini is {}\".format(purity_5, gini_5))\n",
    "print(\"20NEWS, K = 10, purity is {} and gini is {}\".format(purity_10, gini_10))\n",
    "print(\"20NEWS, K = 20, purity is {} and gini is {}\".format(purity_20, gini_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fashion, K = 5, purity is 0.45245 and gini is 0.5932141975998142\n",
      "Fashion, K = 10, purity is 0.58425 and gini is 0.5048468011334925\n",
      "Fashion, K = 20, purity is 0.7068333333333333 and gini is 0.37662570610810964\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "##The “mnist_train_fashion.csv” is downloaded from webpage of this link : https://www.kaggle.com/zalando-research/fashionmnist\n",
    "## here we only use the training data set for running the program\n",
    "\n",
    "# data_array_fashion is to store the training data\n",
    "data_array_fashion = []\n",
    "with open(\"/Users/deshenghu/Dropbox/Dataset_CS6220/mnist_train_fashion.csv\") as t:\n",
    "    #read csv file using csv.reader()\n",
    "    #source : https://docs.python.org/3/library/csv.html\n",
    "    reader = csv.reader(t)\n",
    "    for row in reader:\n",
    "        data_array_fashion.append(row)\n",
    "        \n",
    "\n",
    "# get the lables of the training data, which is the first element of each row\n",
    "data_fashion_lable = [row[0] for row in data_array_fashion]\n",
    "\n",
    "# delete the lable which is the first element of each row\n",
    "for i in range(len(data_array_fashion)):\n",
    "    del data_array_fashion[i][0]\n",
    "    \n",
    "data_fashion_tempt =  np.array(data_array_fashion)\n",
    "# because each element (\"number\") in the array is str and thus need to be converted\n",
    "data_fashion_training = [list(map(int, data_fashion_tempt[j])) for j in range(len(data_fashion_tempt))]\n",
    "\n",
    "## after getting the training data and the corresponding lable, we need to run the k-means function to get the centroids and predicted cluster or centroides\n",
    "cen_fashion_5, cluster_fashion_5 = K_means(data_fashion_training, 5, 50)\n",
    "cen_fashion_10, cluster_fashion_10 = K_means(data_fashion_training, 10, 50)\n",
    "cen_fashion_20, cluster_fashion_20 = K_means(data_fashion_training, 20, 50)\n",
    "\n",
    "\n",
    "purity_5 = compute_purity(data_fashion_training, cen_fashion_5, cluster_fashion_5, data_fashion_lable)\n",
    "gini_5 = compute_gini(data_fashion_training, cen_fashion_5, cluster_fashion_5, data_fashion_lable)\n",
    "purity_10 = compute_purity(data_fashion_training, cen_fashion_10, cluster_fashion_10, data_fashion_lable)\n",
    "gini_10 = compute_gini(data_fashion_training, cen_fashion_10, cluster_fashion_10, data_fashion_lable)\n",
    "purity_20 = compute_purity(data_fashion_training, cen_fashion_20, cluster_fashion_20, data_fashion_lable)\n",
    "gini_20 = compute_gini(data_fashion_training, cen_fashion_20, cluster_fashion_20, data_fashion_lable)\n",
    "\n",
    "print(\"Fashion, K = 5, purity is {} and gini is {}\".format(purity_5, gini_5))\n",
    "print(\"Fashion, K = 10, purity is {} and gini is {}\".format(purity_10, gini_10))\n",
    "print(\"Fashion, K = 20, purity is {} and gini is {}\".format(purity_20, gini_20))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
