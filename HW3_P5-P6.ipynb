{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/deshenghu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "(11314, 129791)\n",
      "(7532, 129791)\n"
     ]
    }
   ],
   "source": [
    "# code source from http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#\n",
    "# code source from http://www.nltk.org/book/ch02.html\n",
    "# code source from http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# extracts the archive contents in the ~/scikit_learn_data/20news_home folder \n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "################# this part is for pre-processing the text file which isn't mandatory\n",
    "# get rid of the stopwords and punctuation\n",
    "nltk.download('stopwords')\n",
    "stop_word =stopwords.words('english')\n",
    "punctuation = string.punctuation\n",
    "stopw_punctuation = list(stop_word) + list(punctuation)\n",
    "\n",
    "# from nltk.tokenize import word_tokenize\n",
    "\n",
    "# # to tokenize the word\n",
    "# for j in range(len(newsgroups_train.data)):\n",
    "#     newsgroups_train.data[j] = \" \".join([w for w in word_tokenize(newsgroups_train.data[j]) if w not in stopw_punctuation])\n",
    "#    # print(newsgroups_train.data[])\n",
    "# for j in range(len(newsgroups_test.data)):\n",
    "#     newsgroups_test.data[j] = \" \".join([w for w in word_tokenize(newsgroups_train.data[j]) if w not in stopw_punctuation])\n",
    "\n",
    "#print(len(newsgroups_train.data))\n",
    "#print(newsgroups_train.data[1])\n",
    "\n",
    "##############\n",
    "\n",
    "# from http://scikit-learn.org/stable/datasets/twenty_newsgroups.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = 'english')\n",
    "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "# the test data set don't need to fit\n",
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, stop_words = 'english', vocabulary = vectorizer.vocabulary_)\n",
    "vectors_test = vectorizer.fit_transform(newsgroups_test.data)\n",
    "print(vectors_train.shape)\n",
    "print(vectors_test.shape)\n",
    "\n",
    "\n",
    "news_train = vectors_train\n",
    "news_test = vectors_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20NG News Training dataset, Accuracy of LR over L1 prior as regularizer(i.e., Lasso) is 0.750132579105533\n",
      "20NG News Test dataset, Accuracy of LR over L1 prior as regularizer(i.e., Lasso) is 0.676048858204992\n"
     ]
    }
   ],
   "source": [
    "## example from link: http://scikit-learn.org/stable/modules/feature_selection.html\n",
    "'''\n",
    ">>> from sklearn.svm import LinearSVC\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.feature_selection import SelectFromModel\n",
    ">>> iris = load_iris()\n",
    ">>> X, y = iris.data, iris.target\n",
    ">>> X.shape\n",
    "(150, 4)\n",
    ">>> lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    ">>> model = SelectFromModel(lsvc, prefit=True)\n",
    ">>> X_new = model.transform(X)\n",
    ">>> X_new.shape\n",
    "(150, 3)\n",
    "'''\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.feature_selection import SelectFromModel as SFM\n",
    "# link: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "# Linear Model trained with L1 prior as regularizer (aka the Lasso)\n",
    "## 20 NG news dataset\n",
    "news_train_label = newsgroups_train.target\n",
    "Lasso = LR(C = 0.3, penalty='l1',).fit(news_train, news_train_label)\n",
    "model = SFM(Lasso, prefit = True)\n",
    "X_train = model.transform(news_train)\n",
    "X_test = model.transform(news_test)\n",
    "\n",
    "news_test_label = newsgroups_test.target\n",
    "LR_lasso = LR().fit(X_train, news_train_label)\n",
    "\n",
    "accuracy_news_train_lasso = LR_lasso.score(X_train, news_train_label)\n",
    "accuracy_news_test_lasso = LR_lasso.score(X_test, news_test_label)\n",
    "\n",
    "print(\"20NG News Training dataset, Accuracy of LR over L1 prior as regularizer(i.e., Lasso) is {}\".format(accuracy_news_train_lasso))\n",
    "print(\"20NG News Test dataset, Accuracy of LR over L1 prior as regularizer(i.e., Lasso) is {}\".format(accuracy_news_test_lasso))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# source: http://scikit-learn.org/stable/datasets/index.html\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist_data = fetch_mldata('MNIST original')['data']\n",
    "\n",
    "#get the labels of the MNIST dataset \n",
    "mnist_label = fetch_mldata('MNIST original')['target']\n",
    "print(mnist_data.shape)\n",
    "\n",
    "## link:https://docs.python.org/3/library/random.html#random.randint\n",
    "from random import randrange\n",
    "\n",
    "'''\n",
    "here is to generate a list of rectangles and each side of each rectangle is least 5 and at most 23\n",
    "'''\n",
    "rectangle_list = []\n",
    "for _ in range(130):\n",
    "    ## rectangle is constrained to have approx 130-170 area, thus each side should be at least 5 and at most 23.\n",
    "    A = randrange(0,23)\n",
    "    B = randrange(0,23)\n",
    "    C = randrange(a+5,28)\n",
    "    D = randrange(b+5,28)\n",
    "    rectangle_list.append([A,B,C,D])\n",
    "\n",
    "'''\n",
    "here function \"get_harr_features\" is to get a list of HARR_feature correponding to the input data set\n",
    "'''\n",
    "def get_harr_features(a, rectangle_list):\n",
    "    \n",
    "    black = np.zeros((28, 28, 28, 28))\n",
    "    ## here is to calculate the sum of a area covering the two diagonal points\n",
    "    for i in range(28):\n",
    "        black[0, 0, 0, i] = sum(a[0, :i])\n",
    "        black[0, 0, i, 0] = sum(a[:i, 0])\n",
    "        \n",
    "    ## here is to calculate the O-cornered rectangles by using dynamic programming, e.g., \n",
    "    '''\n",
    "    for i=rows\n",
    "    for j=columns\n",
    "    black(rectangle-diag(ODij)) = black(rectangle-diag(ODi,j-1)) + black(rectangle-diag(ODi-1,j)) \n",
    "                                - black(rectangle-diag(ODi-1,j-1)) + black(pixel Dij)\n",
    "    '''\n",
    "    for i in range(1,28):\n",
    "        for j in range(1, 28):\n",
    "            black[0, 0, i, j] = black[0, 0, i-1, j] + black[0,0,i,j-1] - black[0,0,i-1,j-1] + a[i,j]\n",
    "            \n",
    "    HARR_feature_list = []\n",
    "    '''\n",
    "    Since all rectangles cornered at O have their black computed and stored, the procedure for general rectangles is to:\n",
    "    black(rectangle ABCD) = black(OTYD) - black(OTXB) - black(OZYC) + black(OZXA)\n",
    "    '''\n",
    "    for a in rectangle_list:\n",
    "        ## here is to calculate the feature value of the four rectangle areas corresponding to the vertical/horizonal views\n",
    "        black[a[0], a[1], (a[0]+a[2])//2, a[3]]=black[0,0,(a[0]+a[2])//2,a[3]]-black[0,0,(a[0]+a[2])//2,a[1]]-black[0,0,a[3],a[0]]+ black[0,0,a[0],a[1]]\n",
    "        black[(a[0]+a[2])//2, a[1], a[2], a[3]]=black[0,0,a[2],a[3]]-black[0,0,a[2],a[1]]-black[0,0,a[3],(a[0]+a[2])//2]+black[0,0,(a[0]+a[2])//2, a[1]]\n",
    "        black[a[0],a[1],a[2],(a[1]+a[3])//2]=black[0,0,a[2],(a[1]+a[3])//2]-black[0,0,a[2],a[1]]-black[0,0,(a[1]+a[3])//2,a[0]]+black[0,0,a[0],a[1]]\n",
    "        black[a[0],(a[1]+a[3])//2,a[2],a[3]]=black[0,0,a[2],a[3]]-black[0,0,a[2],(a[1]+a[3])//2]-black[0,0,a[3],a[0]]+black[0,0,a[0],(a[1]+a[3])//2]\n",
    "        ## the black vertical difference: black(top-half) - black(bottom-half)\n",
    "        vertic = black[a[0],a[1],(a[0]+a[2])//2, a[3]] - black[(a[0]+a[2])//2,a[1],a[2],a[3]]\n",
    "        ## the black horizontal difference: black(left-half) - black(right-half)\n",
    "        horizon= black[a[0],a[1],a[2], (a[1]+a[3])//2] - black[a[0],(a[1]+a[3])//2,a[2],a[3]]\n",
    "        ## append the black vertical difference into the HARR_feature_list\n",
    "        HARR_feature_list.append(vertic)\n",
    "        ## append the black horizontal difference into the HARR_feature_list\n",
    "        HARR_feature_list.append(horizon)\n",
    "        \n",
    "    return HARR_feature_list\n",
    "\n",
    "'''\n",
    "here is to append the HARR features for the whole mnist dataset into a list named \"mnist_harr_feature_list\"\n",
    "'''\n",
    "mnist_harr_feature_list = []\n",
    "for i in range(0,mnist_data.shape[0],1):\n",
    "    mnist_harr_feature_list.append(get_harr_features(mnist_data[i,:].reshape((28,28)),rectangle_list))\n",
    "    \n",
    "'''\n",
    "here is to append the labels for the whole mnist dataset into a list \n",
    "'''\n",
    "mnist_harr_label_list = []\n",
    "for i in range(0,mnist_data.shape[0],1):\n",
    "    mnist_harr_label_list.append(mnist_label[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deshenghu/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset, Accuracy of LR over HARR feature Extraction is 0.9117857142857143\n"
     ]
    }
   ],
   "source": [
    "# For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’\n",
    "# link: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "LR_harr = LR(penalty='l2', solver = 'sag')\n",
    "LR_harr.fit(mnist_harr_feature_list, mnist_harr_label_list)\n",
    "accuracy_mnist_harr= LR_harr.score(mnist_harr_feature_list, mnist_harr_label_list)\n",
    "\n",
    "print(\"MNIST dataset, Accuracy of LR over HARR feature Extraction is {}\".format(accuracy_mnist_harr))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
